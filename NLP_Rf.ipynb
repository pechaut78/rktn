{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAximisation des résultats par Rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "\n",
    "from RktnChallenge.RktnModel import ModelTrainer\n",
    "from RktnChallenge.preprocessing.tokenizeString import tokenizeString\n",
    "from RktnChallenge.preprocessing.filterStopWords import filterStopWords\n",
    "from RktnChallenge.preprocessing.mergeFeatures import mergeFeatures\n",
    "from RktnChallenge.preprocessing.mostOccur import mostOccur\n",
    "from RktnChallenge.preprocessing.Dropper import Dropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RktnModel = ModelTrainer(\"data_tr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m _X \u001b[39m=\u001b[39m RktnModel\u001b[39m.\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mtkn_reduce\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m RktnModel\u001b[39m.\u001b[39;49mcreate_vectorizer(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtfidf\u001b[39;49m\u001b[39m\"\u001b[39;49m, X \u001b[39m=\u001b[39;49m _X, ngram_range\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m X_data \u001b[39m=\u001b[39m RktnModel\u001b[39m.\u001b[39mvectorizer_transform(_X)     \n\u001b[1;32m      7\u001b[0m y \u001b[39m=\u001b[39m RktnModel\u001b[39m.\u001b[39mencodeLabel(\u001b[39m\"\u001b[39m\u001b[39mprdtypecode\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Rktn/RktnChallenge/RktnModel.py:96\u001b[0m, in \u001b[0;36mModelTrainer.create_vectorizer\u001b[0;34m(self, name, X, max_features, ngram_range)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m(name\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer \u001b[39m=\u001b[39m TfidfVectorizer(max_features \u001b[39m=\u001b[39m max_features, ngram_range\u001b[39m=\u001b[39mngram_range)\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorizer\u001b[39m.\u001b[39;49mfit(X)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m(name\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbow\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer \u001b[39m=\u001b[39m CountVectorizer(max_features \u001b[39m=\u001b[39m max_features, ngram_range\u001b[39m=\u001b[39mngram_range)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2096\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2089\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m   2090\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2091\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2092\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2093\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2094\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2095\u001b[0m )\n\u001b[0;32m-> 2096\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2098\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "_X = RktnModel.data[\"tkn_reduce\"]\n",
    "RktnModel.create_vectorizer(name=\"tfidf\", X = _X, ngram_range=(1,3))\n",
    "\n",
    "X_data = RktnModel.vectorizer_transform(_X)     \n",
    "\n",
    "\n",
    "y = RktnModel.encodeLabel(\"prdtypecode\")\n",
    "label_size = RktnModel.getLabelSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size = 0.20, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application d'un random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16984, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.21      0.23       612\n",
      "           1       0.43      0.40      0.41       521\n",
      "           2       0.47      0.33      0.39       357\n",
      "           3       0.35      0.21      0.26       161\n",
      "           4       0.49      0.55      0.52       539\n",
      "           5       0.60      0.59      0.59       786\n",
      "           6       0.42      0.32      0.36       146\n",
      "           7       0.43      0.34      0.38       961\n",
      "           8       0.27      0.22      0.25       424\n",
      "           9       0.48      0.61      0.54       974\n",
      "          10       0.48      0.66      0.56       169\n",
      "          11       0.54      0.36      0.43       507\n",
      "          12       0.55      0.41      0.47       672\n",
      "          13       0.50      0.51      0.50      1013\n",
      "          14       0.72      0.80      0.76       841\n",
      "          15       0.51      0.50      0.50       137\n",
      "          16       0.54      0.54      0.54      1029\n",
      "          17       0.49      0.41      0.44       170\n",
      "          18       0.54      0.60      0.57       942\n",
      "          19       0.48      0.47      0.47       986\n",
      "          20       0.48      0.37      0.42       306\n",
      "          21       0.67      0.72      0.69       991\n",
      "          22       0.41      0.33      0.37       462\n",
      "          23       0.62      0.84      0.71      2047\n",
      "          24       0.44      0.30      0.36       525\n",
      "          25       0.24      0.26      0.25       517\n",
      "          26       0.42      0.27      0.33       189\n",
      "\n",
      "    accuracy                           0.52     16984\n",
      "   macro avg       0.48      0.45      0.46     16984\n",
      "weighted avg       0.51      0.52      0.51     16984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs = -1,n_estimators=300)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    " \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

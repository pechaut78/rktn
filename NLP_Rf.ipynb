{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAximisation des résultats par RfId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Désactiver le GPU en définissant CUDA_VISIBLE_DEVICES à un vide#\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import RktnChallenge.RktnModel\n",
    "importlib.reload(RktnChallenge.RktnModel)\n",
    "from RktnChallenge.RktnModel import ModelTrainer\n",
    "\n",
    "import RktnChallenge.preprocessing.tokenizeString\n",
    "importlib.reload(RktnChallenge.preprocessing.tokenizeString)\n",
    "from RktnChallenge.preprocessing.tokenizeString import tokenizeString\n",
    "\n",
    "import RktnChallenge.preprocessing.filterStopWords\n",
    "importlib.reload(RktnChallenge.preprocessing.filterStopWords)\n",
    "from RktnChallenge.preprocessing.filterStopWords import filterStopWords\n",
    "\n",
    "\n",
    "import RktnChallenge.preprocessing.mergeFeatures\n",
    "importlib.reload(RktnChallenge.preprocessing.mergeFeatures)\n",
    "from RktnChallenge.preprocessing.mergeFeatures import mergeFeatures\n",
    "\n",
    "import RktnChallenge.preprocessing.mostOccur\n",
    "importlib.reload(RktnChallenge.preprocessing.mostOccur)\n",
    "from RktnChallenge.preprocessing.mostOccur import mostOccur\n",
    "\n",
    "import RktnChallenge.preprocessing.Dropper\n",
    "importlib.reload(RktnChallenge.preprocessing.Dropper)\n",
    "from RktnChallenge.preprocessing.Dropper import Dropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RktnModel = ModelTrainer(\"data_tr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>imgname</th>\n",
       "      <th>desi_langue</th>\n",
       "      <th>tr_designation</th>\n",
       "      <th>desc_langue</th>\n",
       "      <th>tr_description</th>\n",
       "      <th>designation_tkn</th>\n",
       "      <th>description_tkn</th>\n",
       "      <th>merged_tkns</th>\n",
       "      <th>tkn_reduce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>image_1263597046_product_3804725264.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[olivia, personalisiertes, notizbuch, 150, sei...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[olivia, personalisiertes, notizbuch, 150, sei...</td>\n",
       "      <td>[olivia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>image_1008141237_product_436067568.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[journal, arts, ndeg, 133, 28, 09, 2001, art, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[journal, arts, ndeg, 133, 28, 09, 2001, art, ...</td>\n",
       "      <td>[art]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>image_938777978_product_201115110.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>fr</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>[pilot, style, touch, pen, marque, speedlink, ...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>[gamepad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>image_457047496_product_50418756.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "      <td>[peluche]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des idees de grandeur. Il veut organiser...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>image_1077757786_product_278535884.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>fr</td>\n",
       "      <td>Luc a des idees de grandeur. Il veut organiser...</td>\n",
       "      <td>[guerre, tuques]</td>\n",
       "      <td>[luc, idees, grandeur, veut, organiser, jeu, g...</td>\n",
       "      <td>[guerre, tuques, luc, idees, grandeur, veut, o...</td>\n",
       "      <td>[guerre]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84911</th>\n",
       "      <td>84911</td>\n",
       "      <td>84911</td>\n",
       "      <td>The Sims [ Import Anglais ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206719094</td>\n",
       "      <td>941495734</td>\n",
       "      <td>40</td>\n",
       "      <td>image_941495734_product_206719094.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>The Sims [ Import Anglais ]</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sims, import, anglais]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[sims, import, anglais]</td>\n",
       "      <td>[sims]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84912</th>\n",
       "      <td>84912</td>\n",
       "      <td>84912</td>\n",
       "      <td>Kit piscine acier NEVADA deco pierre O 3.50m x...</td>\n",
       "      <td>Description complete :Kit piscine hors-sol Toi...</td>\n",
       "      <td>3065095706</td>\n",
       "      <td>1188462883</td>\n",
       "      <td>2583</td>\n",
       "      <td>image_1188462883_product_3065095706.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Kit piscine acier NEVADA deco pierre O 3.50m x...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Description complete :Kit piscine hors-sol Toi...</td>\n",
       "      <td>[kit, piscine, acier, nevada, deco, pierre, 3,...</td>\n",
       "      <td>[description, complete, kit, piscine, hors, so...</td>\n",
       "      <td>[kit, piscine, acier, nevada, deco, pierre, 3,...</td>\n",
       "      <td>[acier]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84913</th>\n",
       "      <td>84913</td>\n",
       "      <td>84913</td>\n",
       "      <td>Journal Officiel De La Republique Francaise Nd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440707564</td>\n",
       "      <td>1009325617</td>\n",
       "      <td>2280</td>\n",
       "      <td>image_1009325617_product_440707564.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Journal Officiel De La Republique Francaise Nd...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[journal, officiel, republique, francaise, nde...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[journal, officiel, republique, francaise, nde...</td>\n",
       "      <td>[partie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84914</th>\n",
       "      <td>84914</td>\n",
       "      <td>84914</td>\n",
       "      <td>Table Basse Bois De Recuperation Massif Base B...</td>\n",
       "      <td>Cette table basse a un design unique et consti...</td>\n",
       "      <td>3942400296</td>\n",
       "      <td>1267353403</td>\n",
       "      <td>1560</td>\n",
       "      <td>image_1267353403_product_3942400296.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Table Basse Bois De Recuperation Massif Base B...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Cette table basse a un design unique et consti...</td>\n",
       "      <td>[table, basse, bois, recuperation, massif, bas...</td>\n",
       "      <td>[cette, table, basse, design, unique, constitu...</td>\n",
       "      <td>[table, basse, bois, recuperation, massif, bas...</td>\n",
       "      <td>[bois]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84915</th>\n",
       "      <td>84915</td>\n",
       "      <td>84915</td>\n",
       "      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57203227</td>\n",
       "      <td>684671297</td>\n",
       "      <td>2522</td>\n",
       "      <td>image_684671297_product_57203227.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[gomme, collection, 2, gommes, pinguin, glace,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[gomme, collection, 2, gommes, pinguin, glace,...</td>\n",
       "      <td>[gomme]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84916 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0  \\\n",
       "0                 0           0   \n",
       "1                 1           1   \n",
       "2                 2           2   \n",
       "3                 3           3   \n",
       "4                 4           4   \n",
       "...             ...         ...   \n",
       "84911         84911       84911   \n",
       "84912         84912       84912   \n",
       "84913         84913       84913   \n",
       "84914         84914       84914   \n",
       "84915         84915       84915   \n",
       "\n",
       "                                             designation  \\\n",
       "0      Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1      Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...   \n",
       "2      Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3      Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                                   La Guerre Des Tuques   \n",
       "...                                                  ...   \n",
       "84911                        The Sims [ Import Anglais ]   \n",
       "84912  Kit piscine acier NEVADA deco pierre O 3.50m x...   \n",
       "84913  Journal Officiel De La Republique Francaise Nd...   \n",
       "84914  Table Basse Bois De Recuperation Massif Base B...   \n",
       "84915  Gomme De Collection 2 Gommes Pinguin Glace Ver...   \n",
       "\n",
       "                                             description   productid  \\\n",
       "0                                                    NaN  3804725264   \n",
       "1                                                    NaN   436067568   \n",
       "2      PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   \n",
       "3                                                    NaN    50418756   \n",
       "4      Luc a des idees de grandeur. Il veut organiser...   278535884   \n",
       "...                                                  ...         ...   \n",
       "84911                                                NaN   206719094   \n",
       "84912  Description complete :Kit piscine hors-sol Toi...  3065095706   \n",
       "84913                                                NaN   440707564   \n",
       "84914  Cette table basse a un design unique et consti...  3942400296   \n",
       "84915                                                NaN    57203227   \n",
       "\n",
       "          imageid  prdtypecode                                  imgname  \\\n",
       "0      1263597046           10  image_1263597046_product_3804725264.jpg   \n",
       "1      1008141237         2280   image_1008141237_product_436067568.jpg   \n",
       "2       938777978           50    image_938777978_product_201115110.jpg   \n",
       "3       457047496         1280     image_457047496_product_50418756.jpg   \n",
       "4      1077757786         2705   image_1077757786_product_278535884.jpg   \n",
       "...           ...          ...                                      ...   \n",
       "84911   941495734           40    image_941495734_product_206719094.jpg   \n",
       "84912  1188462883         2583  image_1188462883_product_3065095706.jpg   \n",
       "84913  1009325617         2280   image_1009325617_product_440707564.jpg   \n",
       "84914  1267353403         1560  image_1267353403_product_3942400296.jpg   \n",
       "84915   684671297         2522     image_684671297_product_57203227.jpg   \n",
       "\n",
       "      desi_langue                                     tr_designation  \\\n",
       "0              fr  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1              fr  Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...   \n",
       "2              fr  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3              fr  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4              fr                               La Guerre Des Tuques   \n",
       "...           ...                                                ...   \n",
       "84911          fr                        The Sims [ Import Anglais ]   \n",
       "84912          fr  Kit piscine acier NEVADA deco pierre O 3.50m x...   \n",
       "84913          fr  Journal Officiel De La Republique Francaise Nd...   \n",
       "84914          fr  Table Basse Bois De Recuperation Massif Base B...   \n",
       "84915          fr  Gomme De Collection 2 Gommes Pinguin Glace Ver...   \n",
       "\n",
       "      desc_langue                                     tr_description  \\\n",
       "0              fr                                                NaN   \n",
       "1              fr                                                NaN   \n",
       "2              fr  PILOT STYLE Touch Pen de marque Speedlink est ...   \n",
       "3              fr                                                NaN   \n",
       "4              fr  Luc a des idees de grandeur. Il veut organiser...   \n",
       "...           ...                                                ...   \n",
       "84911          fr                                                NaN   \n",
       "84912          fr  Description complete :Kit piscine hors-sol Toi...   \n",
       "84913          fr                                                NaN   \n",
       "84914          fr  Cette table basse a un design unique et consti...   \n",
       "84915          fr                                                NaN   \n",
       "\n",
       "                                         designation_tkn  \\\n",
       "0      [olivia, personalisiertes, notizbuch, 150, sei...   \n",
       "1      [journal, arts, ndeg, 133, 28, 09, 2001, art, ...   \n",
       "2      [grand, stylet, ergonomique, bleu, gamepad, ni...   \n",
       "3      [peluche, donald, europe, disneyland, 2000, ma...   \n",
       "4                                       [guerre, tuques]   \n",
       "...                                                  ...   \n",
       "84911                            [sims, import, anglais]   \n",
       "84912  [kit, piscine, acier, nevada, deco, pierre, 3,...   \n",
       "84913  [journal, officiel, republique, francaise, nde...   \n",
       "84914  [table, basse, bois, recuperation, massif, bas...   \n",
       "84915  [gomme, collection, 2, gommes, pinguin, glace,...   \n",
       "\n",
       "                                         description_tkn  \\\n",
       "0                                                     []   \n",
       "1                                                     []   \n",
       "2      [pilot, style, touch, pen, marque, speedlink, ...   \n",
       "3                                                     []   \n",
       "4      [luc, idees, grandeur, veut, organiser, jeu, g...   \n",
       "...                                                  ...   \n",
       "84911                                                 []   \n",
       "84912  [description, complete, kit, piscine, hors, so...   \n",
       "84913                                                 []   \n",
       "84914  [cette, table, basse, design, unique, constitu...   \n",
       "84915                                                 []   \n",
       "\n",
       "                                             merged_tkns tkn_reduce  \n",
       "0      [olivia, personalisiertes, notizbuch, 150, sei...   [olivia]  \n",
       "1      [journal, arts, ndeg, 133, 28, 09, 2001, art, ...      [art]  \n",
       "2      [grand, stylet, ergonomique, bleu, gamepad, ni...  [gamepad]  \n",
       "3      [peluche, donald, europe, disneyland, 2000, ma...  [peluche]  \n",
       "4      [guerre, tuques, luc, idees, grandeur, veut, o...   [guerre]  \n",
       "...                                                  ...        ...  \n",
       "84911                            [sims, import, anglais]     [sims]  \n",
       "84912  [kit, piscine, acier, nevada, deco, pierre, 3,...    [acier]  \n",
       "84913  [journal, officiel, republique, francaise, nde...   [partie]  \n",
       "84914  [table, basse, bois, recuperation, massif, bas...     [bois]  \n",
       "84915  [gomme, collection, 2, gommes, pinguin, glace,...    [gomme]  \n",
       "\n",
       "[84916 rows x 16 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addendum = ['a','à','<p>','<b>','<div>','<em>','<br>']\n",
    "\n",
    "#on tokenise et retire les stopwords designation\n",
    "\n",
    "desi_tokenize= tokenizeString(\"designation_tkn\",\"designation\")\n",
    "desi_stopwords = filterStopWords(\"designation_tkn\", \"designation_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "\n",
    "#on tokenise et retire les stopwords description\n",
    "desc_tokenize= tokenizeString(\"description_tkn\",\"description\")\n",
    "desc_stopwords = filterStopWords(\"description_tkn\", \"description_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_desi_desc = mergeFeatures(\"merged_tkns\",\"designation_tkn\",\"description_tkn\")\n",
    "\n",
    "desc_mostOccur= mostOccur(\"tkn_reduce\",\"merged_tkns\",1,1)\n",
    "\n",
    "toDrop = [\"designation_tkn\",\"description_tkn\",\"desc_reduce\"]\n",
    "cleanDropper = Dropper(column_to_drop = toDrop)\n",
    "\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"tokenize designation\",desi_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter designation\",desi_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"tokenize description\",desc_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter description\",desc_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"merge designation and desc\",merge_desi_desc) # merge desi&desc\n",
    "RktnModel.addPreprocessStep(\"desc reduce\",desc_mostOccur)         # reduction taille desc\n",
    "RktnModel.preprocess()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m _X \u001b[39m=\u001b[39m RktnModel\u001b[39m.\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mtkn_reduce\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m RktnModel\u001b[39m.\u001b[39;49mcreate_vectorizer(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtfidf\u001b[39;49m\u001b[39m\"\u001b[39;49m, X \u001b[39m=\u001b[39;49m _X, ngram_range\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m X_data \u001b[39m=\u001b[39m RktnModel\u001b[39m.\u001b[39mvectorizer_transform(_X)     \n\u001b[1;32m      7\u001b[0m y \u001b[39m=\u001b[39m RktnModel\u001b[39m.\u001b[39mencodeLabel(\u001b[39m\"\u001b[39m\u001b[39mprdtypecode\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Rktn/RktnChallenge/RktnModel.py:96\u001b[0m, in \u001b[0;36mModelTrainer.create_vectorizer\u001b[0;34m(self, name, X, max_features, ngram_range)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m(name\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer \u001b[39m=\u001b[39m TfidfVectorizer(max_features \u001b[39m=\u001b[39m max_features, ngram_range\u001b[39m=\u001b[39mngram_range)\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorizer\u001b[39m.\u001b[39;49mfit(X)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m(name\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbow\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer \u001b[39m=\u001b[39m CountVectorizer(max_features \u001b[39m=\u001b[39m max_features, ngram_range\u001b[39m=\u001b[39mngram_range)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2096\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2089\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m   2090\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2091\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2092\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2093\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2094\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2095\u001b[0m )\n\u001b[0;32m-> 2096\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2098\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "_X = RktnModel.data[\"tkn_reduce\"]\n",
    "RktnModel.create_vectorizer(name=\"tfidf\", X = _X, ngram_range=(1,3))\n",
    "\n",
    "X_data = RktnModel.vectorizer_transform(_X)     \n",
    "\n",
    "\n",
    "y = RktnModel.encodeLabel(\"prdtypecode\")\n",
    "label_size = RktnModel.getLabelSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size = 0.20, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application d'un random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16984, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.21      0.23       612\n",
      "           1       0.43      0.40      0.41       521\n",
      "           2       0.47      0.33      0.39       357\n",
      "           3       0.35      0.21      0.26       161\n",
      "           4       0.49      0.55      0.52       539\n",
      "           5       0.60      0.59      0.59       786\n",
      "           6       0.42      0.32      0.36       146\n",
      "           7       0.43      0.34      0.38       961\n",
      "           8       0.27      0.22      0.25       424\n",
      "           9       0.48      0.61      0.54       974\n",
      "          10       0.48      0.66      0.56       169\n",
      "          11       0.54      0.36      0.43       507\n",
      "          12       0.55      0.41      0.47       672\n",
      "          13       0.50      0.51      0.50      1013\n",
      "          14       0.72      0.80      0.76       841\n",
      "          15       0.51      0.50      0.50       137\n",
      "          16       0.54      0.54      0.54      1029\n",
      "          17       0.49      0.41      0.44       170\n",
      "          18       0.54      0.60      0.57       942\n",
      "          19       0.48      0.47      0.47       986\n",
      "          20       0.48      0.37      0.42       306\n",
      "          21       0.67      0.72      0.69       991\n",
      "          22       0.41      0.33      0.37       462\n",
      "          23       0.62      0.84      0.71      2047\n",
      "          24       0.44      0.30      0.36       525\n",
      "          25       0.24      0.26      0.25       517\n",
      "          26       0.42      0.27      0.33       189\n",
      "\n",
      "    accuracy                           0.52     16984\n",
      "   macro avg       0.48      0.45      0.46     16984\n",
      "weighted avg       0.51      0.52      0.51     16984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs = -1,n_estimators=300)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    " \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

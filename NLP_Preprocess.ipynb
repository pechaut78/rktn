{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation de data_tr_lemm.csv\n",
    "- Prend data_tr en entrée (= traduction de data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Désactiver le GPU en définissant CUDA_VISIBLE_DEVICES à un vide#\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import RktnChallenge.RktnModel\n",
    "importlib.reload(RktnChallenge.RktnModel)\n",
    "from RktnChallenge.RktnModel import ModelTrainer\n",
    "\n",
    "\n",
    "from RktnChallenge.preprocessing.tokenizeString import tokenizeString\n",
    "from RktnChallenge.preprocessing.filterStopWords import filterStopWords\n",
    "from RktnChallenge.preprocessing.mergeFeatures import mergeFeatures\n",
    "from RktnChallenge.preprocessing.mostOccur import mostOccur\n",
    "from RktnChallenge.preprocessing.Dropper import Dropper\n",
    "from RktnChallenge.preprocessing.TokenListToString import TokenListToString\n",
    "\n",
    "\n",
    "\n",
    "import RktnChallenge.preprocessing.filterChar\n",
    "importlib.reload(RktnChallenge.preprocessing.filterChar)\n",
    "from RktnChallenge.preprocessing.filterChar import filterChar\n",
    "\n",
    "import RktnChallenge.preprocessing.lemmatize\n",
    "importlib.reload(RktnChallenge.preprocessing.lemmatize)\n",
    "from RktnChallenge.preprocessing.lemmatize import lemmatize\n",
    "\n",
    "import RktnChallenge.preprocessing.regularExprSub\n",
    "importlib.reload(RktnChallenge.preprocessing.regularExprSub)\n",
    "from RktnChallenge.preprocessing.regularExprSub import regularExprSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RktnModel = ModelTrainer(\"data_tr.csv\")\n",
    "nom_sauvegarde = \"data_tr_lemm.csv\"\n",
    "nom_sauvegarde_sans_lemm = \"data_tr_brut.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/pec/miniconda3/envs/tfGPU/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "addendum = ['a','à','<p>','<b>','<div>','<em>','<br>','gt',\n",
    "            'h','b','k','a','c','d','e','ex','Ndeg']\n",
    "\n",
    "filtr = ['+','-','/','(',')',':',\"'\",'\"',':','@''!','|','#','<','>','?',\n",
    "         '1','2','3','4','5','6','7','8','9','0']\n",
    "\n",
    "#on tokenise et retire les stopwords designation\n",
    "\n",
    "desi_filter = filterChar(\"designation\",\"tr_designation\", filtr)\n",
    "\n",
    "desi_tokenize= tokenizeString(\"designation_tkn\",\"designation\")\n",
    "desi_stopwords = filterStopWords(\"designation_tkn\", \"designation_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "desi_lemmatize = lemmatize(\"designation_tkn\",\"designation_tkn\")\n",
    "\n",
    "#on tokenise et retire les stopwords description\n",
    "desc_filter = filterChar(\"description\",\"tr_description\", filtr)\n",
    "desc_tokenize= tokenizeString(\"description_tkn\",\"description\")\n",
    "desc_stopwords = filterStopWords(\"description_tkn\", \"description_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "desc_lemmatize = lemmatize(\"description_tkn\",\"description_tkn\")\n",
    "merge_desi_desc = mergeFeatures(\"merged\",\"designation_tkn\",\"description_tkn\")\n",
    "desc_mostOccur= mostOccur(\"merged_desi_desc\",\"merged\",1000,1000)\n",
    "\n",
    "\n",
    "\n",
    "toDrop = [\"designation_tkn\",\"description_tkn\",\"merged\"]\n",
    "cleanDropper = Dropper(column_to_drop = toDrop)\n",
    "toString = TokenListToString(\"merged_desi_desc\",\"merged_desi_desc\")\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"clean designation\",desi_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize designation\",desi_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter designation\",desi_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"lemmitize designation\",desi_lemmatize)  # Lemmatisation designation\n",
    "\n",
    "RktnModel.addPreprocessStep(\"clean description\",desc_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize description\",desc_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter description\",desc_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"lemmitize description\",desc_lemmatize)  # Lemmatisation\n",
    "RktnModel.addPreprocessStep(\"merge designation and desc\",merge_desi_desc) # merge desi&desc en un seul champ\n",
    "RktnModel.addPreprocessStep(\"desc reduce\",desc_mostOccur)         # reduction taille desc\n",
    "RktnModel.addPreprocessStep(\"tostring\",toString) # convertit en chaine de caracteres\n",
    "\n",
    "\n",
    "#RktnModel.addPreprocessStep(\"cleaning\",cleanDropper) # on clean les colonnes crées\n",
    "RktnModel.preprocess()\n",
    "\n",
    "removeNumbers = regularExprSub(\"merged_desi_desc\",\"merged_desi_desc\",filtr='\\d+')\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"removeNumbers\",removeNumbers) # on enleve les nombres qui pourraient rester\n",
    "RktnModel.preprocess()\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentative de sauvegarde avec et sans lemmatization, et crop sur description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "addendum = ['a','à','<p>','<b>','<div>','<em>','<br>','gt',\n",
    "            'h','b','k','a','c','d','e','ex','-','ndeg','(',')','r',\".\",'\"',\"*\",'/',\"ndeg1\",\"ndgeg126\",\"!\",\":\",\"ndeg111\"]\n",
    "\n",
    "filtr = ['+','-','/','(',')',\"'\",'\"',':','@','!','|','#','<','>','?']\n",
    "\n",
    "lang = ['danish', 'dutch', 'english', 'finnish', \n",
    "    'french', 'german', 'indonesian', 'italian','norwegian', 'portuguese', 'spanish', 'swedish', \n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "#on tokenise et retire les stopwords designation\n",
    "\n",
    "desi_filter = filterChar(\"designation\",\"tr_designation\", filtr)\n",
    "\n",
    "desi_tokenize= tokenizeString(\"designation_tkn\",\"designation\")\n",
    "desi_stopwords = filterStopWords(\"designation_tkn\", \"designation_tkn\"\n",
    "                                 ,lang=lang,addendum=addendum)\n",
    "\n",
    "desi_lemmatize = lemmatize(\"designation_tkn_lemm\",\"designation_tkn\")\n",
    "\n",
    "#on tokenise et retire les stopwords description\n",
    "desc_filter = filterChar(\"description\",\"tr_description\", filtr)\n",
    "desc_tokenize= tokenizeString(\"description_tkn\",\"description\")\n",
    "desc_stopwords = filterStopWords(\"description_tkn\", \"description_tkn\"\n",
    "                                 ,lang=lang,addendum=addendum)\n",
    "\n",
    "desc_lemmatize = lemmatize(\"description_tkn_lemm\",\"description_tkn\")\n",
    "\n",
    "merge_desi_desc = mergeFeatures(\"merged\",\"designation_tkn_lemm\",\"description_tkn_lemm\")\n",
    "desc_mostOccur= mostOccur(\"merged_desi_desc\",\"merged\",700,1000)\n",
    "desc_tkn_lemm_crop_mostOccur= mostOccur(\"description_tkn_lemm_crop\",\"description_tkn_lemm\",350,400)\n",
    "desc_tkn_crop_mostOccur= mostOccur(\"description_tkn_crop\",\"description_tkn\",350,400)\n",
    "\n",
    "\n",
    "#toDrop = [\"designation_tkn\",\"description_tkn\",\"merged\"]\n",
    "#cleanDropper = Dropper(column_to_drop = toDrop)\n",
    "toString = TokenListToString(\"merged_desi_desc\",\"merged_desi_desc\")\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"clean designation\",desi_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize designation\",desi_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter designation\",desi_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"lemmitize designation\",desi_lemmatize)  # Lemmatisation designation\n",
    "\n",
    "RktnModel.addPreprocessStep(\"clean description\",desc_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize description\",desc_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter description\",desc_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"lemmitize description\",desc_lemmatize)  # Lemmatisation\n",
    "RktnModel.addPreprocessStep(\"merge designation and desc\",merge_desi_desc) # merge desi&desc en un seul champ\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.addPreprocessStep(\"desc reduce\",desc_mostOccur)         # reduction taille desc\n",
    "RktnModel.addPreprocessStep(\"desc tkn lemm reduce\",desc_tkn_lemm_crop_mostOccur) # merge desi&desc en un seul champ\n",
    "RktnModel.addPreprocessStep(\"desc tkn reduce\",desc_tkn_crop_mostOccur) # merge desi&desc en un seul champ\n",
    "\n",
    "RktnModel.addPreprocessStep(\"tostring\",toString) # convertit en chaine de caracteres\n",
    "\n",
    "RktnModel.preprocess()\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>imgname</th>\n",
       "      <th>desi_langue</th>\n",
       "      <th>tr_designation</th>\n",
       "      <th>desc_langue</th>\n",
       "      <th>tr_description</th>\n",
       "      <th>designation_tkn</th>\n",
       "      <th>designation_tkn_lemm</th>\n",
       "      <th>description_tkn</th>\n",
       "      <th>description_tkn_lemm</th>\n",
       "      <th>merged</th>\n",
       "      <th>merged_desi_desc</th>\n",
       "      <th>description_tkn_lemm_crop</th>\n",
       "      <th>description_tkn_crop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Olivia  cahier personnalisé   150 pages   gril...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>image_1263597046_product_3804725264.jpg</td>\n",
       "      <td>de</td>\n",
       "      <td>Olivia: cahier personnalisé / 150 pages / gril...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[olivia, cahier, personnalisé, 150, pages, gri...</td>\n",
       "      <td>[olivia, cahier, personnaliser, 150, page, gri...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[olivia, cahier, personnaliser, 150, page, gri...</td>\n",
       "      <td>olivia cahier personnaliser 150 page grille po...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts  Le  Ndeg 133 Du 28 09 2001  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>image_1008141237_product_436067568.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[journal, arts, 133, 28, 09, 2001, art, marche...</td>\n",
       "      <td>[journal, art, 133, 28, 09, 2001, art, marche,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[journal, art, 133, 28, 09, 2001, art, marche,...</td>\n",
       "      <td>journal art 133 28 09 2001 art marche salon ar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>image_938777978_product_201115110.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>fr</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>[pilot, style, touch, pen, marque, speedlink, ...</td>\n",
       "      <td>[pilot, style, touch, pen, marque, speedlink, ...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>[pilot, style, touch, pen, marque, speedlink, ...</td>\n",
       "      <td>[pilot, style, touch, pen, marque, speedlink, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Peluche Donald   Europe   Disneyland 2000  Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>image_457047496_product_50418756.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "      <td>peluche donald europe disneyland 2000 marionne...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des idees de grandeur. Il veut organiser...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>image_1077757786_product_278535884.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>fr</td>\n",
       "      <td>Luc a des idees de grandeur. Il veut organiser...</td>\n",
       "      <td>[guerre, tuques]</td>\n",
       "      <td>[guerre, tuque]</td>\n",
       "      <td>[luc, idees, grandeur, veut, organiser, jeu, g...</td>\n",
       "      <td>[luc, idees, grandeur, vouloir, organiser, jeu...</td>\n",
       "      <td>[guerre, tuque, luc, idees, grandeur, vouloir,...</td>\n",
       "      <td>guerre tuque luc idees grandeur vouloir organi...</td>\n",
       "      <td>[luc, idees, grandeur, vouloir, organiser, jeu...</td>\n",
       "      <td>[luc, idees, grandeur, veut, organiser, jeu, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                         designation  \\\n",
       "0  Olivia  cahier personnalisé   150 pages   gril...   \n",
       "1  Journal Des Arts  Le  Ndeg 133 Du 28 09 2001  ...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald   Europe   Disneyland 2000  Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des idees de grandeur. Il veut organiser...   278535884  1077757786   \n",
       "\n",
       "   prdtypecode                                  imgname desi_langue  \\\n",
       "0           10  image_1263597046_product_3804725264.jpg          de   \n",
       "1         2280   image_1008141237_product_436067568.jpg          fr   \n",
       "2           50    image_938777978_product_201115110.jpg          fr   \n",
       "3         1280     image_457047496_product_50418756.jpg          fr   \n",
       "4         2705   image_1077757786_product_278535884.jpg          fr   \n",
       "\n",
       "                                      tr_designation desc_langue  \\\n",
       "0  Olivia: cahier personnalisé / 150 pages / gril...          fr   \n",
       "1  Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...          fr   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...          fr   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...          fr   \n",
       "4                               La Guerre Des Tuques          fr   \n",
       "\n",
       "                                      tr_description  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   \n",
       "3                                                NaN   \n",
       "4  Luc a des idees de grandeur. Il veut organiser...   \n",
       "\n",
       "                                     designation_tkn  \\\n",
       "0  [olivia, cahier, personnalisé, 150, pages, gri...   \n",
       "1  [journal, arts, 133, 28, 09, 2001, art, marche...   \n",
       "2  [grand, stylet, ergonomique, bleu, gamepad, ni...   \n",
       "3  [peluche, donald, europe, disneyland, 2000, ma...   \n",
       "4                                   [guerre, tuques]   \n",
       "\n",
       "                                designation_tkn_lemm  \\\n",
       "0  [olivia, cahier, personnaliser, 150, page, gri...   \n",
       "1  [journal, art, 133, 28, 09, 2001, art, marche,...   \n",
       "2  [grand, stylet, ergonomique, bleu, gamepad, ni...   \n",
       "3  [peluche, donald, europe, disneyland, 2000, ma...   \n",
       "4                                    [guerre, tuque]   \n",
       "\n",
       "                                     description_tkn  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [pilot, style, touch, pen, marque, speedlink, ...   \n",
       "3                                                 []   \n",
       "4  [luc, idees, grandeur, veut, organiser, jeu, g...   \n",
       "\n",
       "                                description_tkn_lemm  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [pilot, style, touch, pen, marque, speedlink, ...   \n",
       "3                                                 []   \n",
       "4  [luc, idees, grandeur, vouloir, organiser, jeu...   \n",
       "\n",
       "                                              merged  \\\n",
       "0  [olivia, cahier, personnaliser, 150, page, gri...   \n",
       "1  [journal, art, 133, 28, 09, 2001, art, marche,...   \n",
       "2  [grand, stylet, ergonomique, bleu, gamepad, ni...   \n",
       "3  [peluche, donald, europe, disneyland, 2000, ma...   \n",
       "4  [guerre, tuque, luc, idees, grandeur, vouloir,...   \n",
       "\n",
       "                                    merged_desi_desc  \\\n",
       "0  olivia cahier personnaliser 150 page grille po...   \n",
       "1  journal art 133 28 09 2001 art marche salon ar...   \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...   \n",
       "3  peluche donald europe disneyland 2000 marionne...   \n",
       "4  guerre tuque luc idees grandeur vouloir organi...   \n",
       "\n",
       "                           description_tkn_lemm_crop  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [pilot, style, touch, pen, marque, speedlink, ...   \n",
       "3                                                 []   \n",
       "4  [luc, idees, grandeur, vouloir, organiser, jeu...   \n",
       "\n",
       "                                description_tkn_crop  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [pilot, style, touch, pen, marque, speedlink, ...  \n",
       "3                                                 []  \n",
       "4  [luc, idees, grandeur, veut, organiser, jeu, g...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(RktnModel.data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "RktnModel = ModelTrainer(nom_sauvegarde)\n",
    "RktnModel.data['description_length'] = RktnModel.data['description'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "RktnModel.data['designation_length'] = RktnModel.data['designation'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "\n",
    "min_val = RktnModel.data['description_length'].min()\n",
    "max_val = RktnModel.data['description_length'].max()\n",
    "RktnModel.data['description_length_normalized'] = (RktnModel.data['description_length'] - min_val) / (max_val - min_val)\n",
    "\n",
    "min_val = RktnModel.data['designation_length'].min()\n",
    "max_val = RktnModel.data['designation_length'].max()\n",
    "RktnModel.data['designation_length_normalized'] = (RktnModel.data['designation_length'] - min_val) / (max_val - min_val)\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pec/Rktn/NLP_Preprocess.ipynb Cellule 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pec/Rktn/NLP_Preprocess.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m RktnModel\u001b[39m.\u001b[39maddPreprocessStep(\u001b[39m\"\u001b[39m\u001b[39mtostring\u001b[39m\u001b[39m\"\u001b[39m,toString) \u001b[39m# convertit en chaine de caracteres\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pec/Rktn/NLP_Preprocess.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m#RktnModel.addPreprocessStep(\"cleaning\",cleanDropper) # on clean les colonnes crées\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pec/Rktn/NLP_Preprocess.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m RktnModel\u001b[39m.\u001b[39;49mpreprocess()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pec/Rktn/NLP_Preprocess.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m removeNumbers \u001b[39m=\u001b[39m regularExprSub(\u001b[39m\"\u001b[39m\u001b[39mmerged_desi_desc\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mmerged_desi_desc\u001b[39m\u001b[39m\"\u001b[39m,filtr\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pec/Rktn/NLP_Preprocess.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m RktnModel\u001b[39m.\u001b[39minitPreprocess()\n",
      "File \u001b[0;32m~/Rktn/RktnChallenge/RktnModel.py:86\u001b[0m, in \u001b[0;36mModelTrainer.preprocess\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     84\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess \u001b[39m=\u001b[39m Pipeline(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_steps, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[39m# _preprocess.fit(X)\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m  Y\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m     87\u001b[0m  \u001b[39mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/pipeline.py:689\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    687\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    688\u001b[0m \u001b[39mfor\u001b[39;00m _, _, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter():\n\u001b[0;32m--> 689\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m Xt\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Rktn/RktnChallenge/preprocessing/filterStopWords.py:29\u001b[0m, in \u001b[0;36mfilterStopWords.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     26\u001b[0m     y \u001b[39m=\u001b[39m [word\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m x]\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m([word\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m y \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words])\n\u001b[0;32m---> 29\u001b[0m X[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdest] \u001b[39m=\u001b[39m X[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrc]\u001b[39m.\u001b[39;49mapply(filter_stopword)\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfGPU/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Rktn/RktnChallenge/preprocessing/filterStopWords.py:27\u001b[0m, in \u001b[0;36mfilterStopWords.transform.<locals>.filter_stopword\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter_stopword\u001b[39m(x):\n\u001b[1;32m     26\u001b[0m     y \u001b[39m=\u001b[39m [word\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m x]\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m([word\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m y \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words])\n",
      "File \u001b[0;32m~/Rktn/RktnChallenge/preprocessing/filterStopWords.py:27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter_stopword\u001b[39m(x):\n\u001b[1;32m     26\u001b[0m     y \u001b[39m=\u001b[39m [word\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m x]\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m([word\u001b[39m.\u001b[39;49mlower() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m y \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "addendum = ['a','à','<p>','<b>','<div>','<em>','<br>','gt',\n",
    "            'h','b','k','a','c','d','e','ex','Ndeg']\n",
    "\n",
    "filtr = ['+','-','/','(',')',':',\"'\",'\"',':','@''!','|','#','<','>','?',\n",
    "         '1','2','3','4','5','6','7','8','9','0']\n",
    "\n",
    "#on tokenise et retire les stopwords designation\n",
    "\n",
    "desi_filter = filterChar(\"designation\",\"tr_designation\", filtr)\n",
    "\n",
    "desi_tokenize= tokenizeString(\"designation_tkn\",\"designation\")\n",
    "desi_stopwords = filterStopWords(\"designation_tkn\", \"designation_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "#desi_lemmatize = lemmatize(\"designation_tkn\",\"designation_tkn\")\n",
    "\n",
    "#on tokenise et retire les stopwords description\n",
    "desc_filter = filterChar(\"description\",\"tr_description\", filtr)\n",
    "desc_tokenize= tokenizeString(\"description_tkn\",\"description\")\n",
    "desc_stopwords = filterStopWords(\"description_tkn\", \"description_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "#desc_lemmatize = lemmatize(\"description_tkn\",\"description_tkn\")\n",
    "merge_desi_desc = mergeFeatures(\"merged\",\"designation_tkn\",\"description_tkn\")\n",
    "desc_mostOccur= mostOccur(\"merged_desi_desc\",\"merged\",1000,1000)\n",
    "\n",
    "\n",
    "\n",
    "toDrop = [\"designation_tkn\",\"description_tkn\",\"merged\"]\n",
    "cleanDropper = Dropper(column_to_drop = toDrop)\n",
    "toString = TokenListToString(\"merged_desi_desc\",\"merged_desi_desc\")\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"clean designation\",desi_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize designation\",desi_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter designation\",desi_stopwords)  # filtrage des stopwords\n",
    "#RktnModel.addPreprocessStep(\"lemmitize designation\",desi_lemmatize)  # Lemmatisation designation\n",
    "\n",
    "RktnModel.addPreprocessStep(\"clean description\",desc_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize description\",desc_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter description\",desc_stopwords)  # filtrage des stopwords\n",
    "#RktnModel.addPreprocessStep(\"lemmitize description\",desc_lemmatize)  # Lemmatisation\n",
    "RktnModel.addPreprocessStep(\"merge designation and desc\",merge_desi_desc) # merge desi&desc en un seul champ\n",
    "RktnModel.addPreprocessStep(\"desc reduce\",desc_mostOccur)         # reduction taille desc\n",
    "RktnModel.addPreprocessStep(\"tostring\",toString) # convertit en chaine de caracteres\n",
    "\n",
    "#RktnModel.addPreprocessStep(\"cleaning\",cleanDropper) # on clean les colonnes crées\n",
    "RktnModel.preprocess()\n",
    "\n",
    "removeNumbers = regularExprSub(\"merged_desi_desc\",\"merged_desi_desc\",filtr='\\d+')\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"removeNumbers\",removeNumbers) # on enleve les nombres qui pourraient rester\n",
    "RktnModel.preprocess()\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde_sans_lemm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation de data_tr_lemm.csv\n",
    "- Prend data_tr en entrée (= traduction de data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Désactiver le GPU en définissant CUDA_VISIBLE_DEVICES à un vide#\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 13:11:45.571940: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-08 13:11:46.138412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-08 13:11:47.644133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-08 13:11:54.303668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-08 13:11:54.314144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-08 13:11:54.314478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import RktnChallenge.RktnModel\n",
    "importlib.reload(RktnChallenge.RktnModel)\n",
    "from RktnChallenge.RktnModel import ModelTrainer\n",
    "\n",
    "\n",
    "from RktnChallenge.preprocessing.tokenizeString import tokenizeString\n",
    "from RktnChallenge.preprocessing.filterStopWords import filterStopWords\n",
    "from RktnChallenge.preprocessing.mergeFeatures import mergeFeatures\n",
    "from RktnChallenge.preprocessing.mostOccur import mostOccur\n",
    "from RktnChallenge.preprocessing.Dropper import Dropper\n",
    "from RktnChallenge.preprocessing.TokenListToString import TokenListToString\n",
    "\n",
    "\n",
    "\n",
    "import RktnChallenge.preprocessing.filterChar\n",
    "importlib.reload(RktnChallenge.preprocessing.filterChar)\n",
    "from RktnChallenge.preprocessing.filterChar import filterChar\n",
    "\n",
    "import RktnChallenge.preprocessing.lemmatize\n",
    "importlib.reload(RktnChallenge.preprocessing.lemmatize)\n",
    "from RktnChallenge.preprocessing.lemmatize import lemmatize\n",
    "\n",
    "import RktnChallenge.preprocessing.regularExprSub\n",
    "importlib.reload(RktnChallenge.preprocessing.regularExprSub)\n",
    "from RktnChallenge.preprocessing.regularExprSub import regularExprSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RktnModel = ModelTrainer(\"data_tr.csv\")\n",
    "nom_sauvegarde = \"data_tr_lemm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "addendum = ['a','à','<p>','<b>','<div>','<em>','<br>','gt',\n",
    "            'h','b','k','a','c','d','e','ex','Ndeg']\n",
    "\n",
    "filtr = ['+','-','/','(',')',':',\"'\",'\"',':','@''!','|','#','<','>','?',\n",
    "         '1','2','3','4','5','6','7','8','9','0']\n",
    "\n",
    "#on tokenise et retire les stopwords designation\n",
    "\n",
    "desi_filter = filterChar(\"designation\",\"designation\", filtr)\n",
    "\n",
    "desi_tokenize= tokenizeString(\"designation_tkn\",\"designation\")\n",
    "desi_stopwords = filterStopWords(\"designation_tkn\", \"designation_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "desi_lemmatize = lemmatize(\"designation_tkn\",\"designation_tkn\")\n",
    "\n",
    "#on tokenise et retire les stopwords description\n",
    "desc_filter = filterChar(\"description\",\"description\", filtr)\n",
    "desc_tokenize= tokenizeString(\"description_tkn\",\"description\")\n",
    "desc_stopwords = filterStopWords(\"description_tkn\", \"description_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "desc_lemmatize = lemmatize(\"description_tkn\",\"description_tkn\")\n",
    "merge_desi_desc = mergeFeatures(\"merged\",\"designation_tkn\",\"description_tkn\")\n",
    "desc_mostOccur= mostOccur(\"merged_desi_desc\",\"merged\",1000,1000)\n",
    "\n",
    "\n",
    "\n",
    "toDrop = [\"designation_tkn\",\"description_tkn\",\"merged\"]\n",
    "cleanDropper = Dropper(column_to_drop = toDrop)\n",
    "toString = TokenListToString(\"merged_desi_desc\",\"merged_desi_desc\")\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"clean designation\",desi_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize designation\",desi_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter designation\",desi_stopwords)  # filtrage des stopwords\n",
    "#RktnModel.addPreprocessStep(\"lemmitize designation\",desi_lemmatize)  # Lemmatisation designation\n",
    "\n",
    "RktnModel.addPreprocessStep(\"clean description\",desc_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize description\",desc_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter description\",desc_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"lemmitize description\",desc_lemmatize)  # Lemmatisation\n",
    "RktnModel.addPreprocessStep(\"merge designation and desc\",merge_desi_desc) # merge desi&desc en un seul champ\n",
    "RktnModel.addPreprocessStep(\"desc reduce\",desc_mostOccur)         # reduction taille desc\n",
    "RktnModel.addPreprocessStep(\"tostring\",toString) # convertit en chaine de caracteres\n",
    "\n",
    "\n",
    "#RktnModel.addPreprocessStep(\"cleaning\",cleanDropper) # on clean les colonnes crées\n",
    "RktnModel.preprocess()\n",
    "\n",
    "removeNumbers = regularExprSub(\"merged_desi_desc\",\"merged_desi_desc\",filtr='\\d+')\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"removeNumbers\",removeNumbers) # on enleve les nombres qui pourraient rester\n",
    "RktnModel.preprocess()\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83493, 27)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "RktnModel = ModelTrainer(\"data_tr_lemm.csv\")\n",
    "RktnModel.data['description_length'] = RktnModel.data['description'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "RktnModel.data['designation_length'] = RktnModel.data['designation'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "\n",
    "min_val = RktnModel.data['description_length'].min()\n",
    "max_val = RktnModel.data['description_length'].max()\n",
    "RktnModel.data['description_length_normalized'] = (RktnModel.data['description_length'] - min_val) / (max_val - min_val)\n",
    "\n",
    "min_val = RktnModel.data['designation_length'].min()\n",
    "max_val = RktnModel.data['designation_length'].max()\n",
    "RktnModel.data['designation_length_normalized'] = (RktnModel.data['designation_length'] - min_val) / (max_val - min_val)\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

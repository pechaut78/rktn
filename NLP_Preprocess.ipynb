{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation de data_tr_lemm.csv\n",
    "- Prend data_tr en entrée (= traduction de data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Désactiver le GPU en définissant CUDA_VISIBLE_DEVICES à un vide#\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 10:48:14.060112: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-02 10:48:14.084431: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9511] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-02 10:48:14.084455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-02 10:48:14.084470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-02 10:48:14.089576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 10:48:14.617152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-02 10:48:18.908732: I external/local_xla/xla/stream_executor/cuda/cuda_gpu_executor.cc:885] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-02 10:48:18.920435: I external/local_xla/xla/stream_executor/cuda/cuda_gpu_executor.cc:885] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-02 10:48:18.920821: I external/local_xla/xla/stream_executor/cuda/cuda_gpu_executor.cc:885] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import RktnChallenge.RktnModel\n",
    "importlib.reload(RktnChallenge.RktnModel)\n",
    "from RktnChallenge.RktnModel import ModelTrainer\n",
    "\n",
    "\n",
    "from RktnChallenge.preprocessing.tokenizeString import tokenizeString\n",
    "from RktnChallenge.preprocessing.filterStopWords import filterStopWords\n",
    "from RktnChallenge.preprocessing.mergeFeatures import mergeFeatures\n",
    "from RktnChallenge.preprocessing.mostOccur import mostOccur\n",
    "from RktnChallenge.preprocessing.Dropper import Dropper\n",
    "from RktnChallenge.preprocessing.TokenListToString import TokenListToString\n",
    "\n",
    "\n",
    "\n",
    "import RktnChallenge.preprocessing.filterChar\n",
    "importlib.reload(RktnChallenge.preprocessing.filterChar)\n",
    "from RktnChallenge.preprocessing.filterChar import filterChar\n",
    "\n",
    "import RktnChallenge.preprocessing.lemmatize\n",
    "importlib.reload(RktnChallenge.preprocessing.lemmatize)\n",
    "from RktnChallenge.preprocessing.lemmatize import lemmatize\n",
    "\n",
    "import RktnChallenge.preprocessing.regularExprSub\n",
    "importlib.reload(RktnChallenge.preprocessing.regularExprSub)\n",
    "from RktnChallenge.preprocessing.regularExprSub import regularExprSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RktnModel = ModelTrainer(\"data_tr_08_.csv\")\n",
    "nom_sauvegarde = \"data_tr_lemm_.csv\"\n",
    "nom_sauvegarde_sans_lemm = \"data_tr_brut.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Complet, si l'on veut tester avec lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/pec/miniconda3/envs/tfGPU/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "addendum = ['a','à','<p>','<b>','<div>','<em>','<br>','gt',\n",
    "            'h','b','k','a','c','d','e','ex','Ndeg']\n",
    "\n",
    "filtr = ['+','-','/','(',')',':',\"'\",'\"',':','@''!','|','#','<','>','?',\n",
    "         '1','2','3','4','5','6','7','8','9','0']\n",
    "\n",
    "#on tokenise et retire les stopwords designation\n",
    "\n",
    "desi_filter = filterChar(\"designation\",\"tr_designation\", filtr)\n",
    "\n",
    "desi_tokenize= tokenizeString(\"designation_tkn\",\"designation\")\n",
    "desi_stopwords = filterStopWords(\"designation_tkn\", \"designation_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "desi_lemmatize = lemmatize(\"designation_tkn\",\"designation_tkn\")\n",
    "\n",
    "#on tokenise et retire les stopwords description\n",
    "desc_filter = filterChar(\"description\",\"tr_description\", filtr)\n",
    "desc_tokenize= tokenizeString(\"description_tkn\",\"description\")\n",
    "desc_stopwords = filterStopWords(\"description_tkn\", \"description_tkn\"\n",
    "                                 ,lang=[\"english\",\"french\"],addendum=addendum)\n",
    "\n",
    "desc_lemmatize = lemmatize(\"description_tkn\",\"description_tkn\")\n",
    "merge_desi_desc = mergeFeatures(\"merged\",\"designation_tkn\",\"description_tkn\")\n",
    "desc_mostOccur= mostOccur(\"merged_desi_desc\",\"merged\",1000,1000)\n",
    "\n",
    "\n",
    "\n",
    "toDrop = [\"designation_tkn\",\"description_tkn\",\"merged\"]\n",
    "cleanDropper = Dropper(column_to_drop = toDrop)\n",
    "toString = TokenListToString(\"merged_desi_desc\",\"merged_desi_desc\")\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"clean designation\",desi_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize designation\",desi_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter designation\",desi_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"lemmitize designation\",desi_lemmatize)  # Lemmatisation designation\n",
    "\n",
    "RktnModel.addPreprocessStep(\"clean description\",desc_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize description\",desc_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter description\",desc_stopwords)  # filtrage des stopwords\n",
    "RktnModel.addPreprocessStep(\"lemmitize description\",desc_lemmatize)  # Lemmatisation\n",
    "RktnModel.addPreprocessStep(\"merge designation and desc\",merge_desi_desc) # merge desi&desc en un seul champ\n",
    "RktnModel.addPreprocessStep(\"desc reduce\",desc_mostOccur)         # reduction taille desc\n",
    "RktnModel.addPreprocessStep(\"tostring\",toString) # convertit en chaine de caracteres\n",
    "\n",
    "\n",
    "#RktnModel.addPreprocessStep(\"cleaning\",cleanDropper) # on clean les colonnes crées\n",
    "RktnModel.preprocess()\n",
    "\n",
    "removeNumbers = regularExprSub(\"merged_desi_desc\",\"merged_desi_desc\",filtr='\\d+')\n",
    "RktnModel.initPreprocess()\n",
    "RktnModel.addPreprocessStep(\"removeNumbers\",removeNumbers) # on enleve les nombres qui pourraient rester\n",
    "RktnModel.preprocess()\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentative de sauvegarde avec et sans lemmatization, et crop sur description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "addendum = ['a','à','<p>','<b>','<div>','<em>','<br>','gt',\n",
    "            'h','b','k','a','c','d','e','ex','-','ndeg','(',')','r',\".\",'\"',\"*\",'/',\"ndeg1\",\"ndgeg126\",\"!\",\":\",\"ndeg111\"]\n",
    "\n",
    "filtr = ['+','-','/','(',')',\"'\",'\"',':','@','!','|','#','<','>','?']\n",
    "\n",
    "lang = ['english','french', \n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "#on tokenise et retire les stopwords designation\n",
    "\n",
    "desi_tokenize= tokenizeString(\"designation_tkn\",\"tr_designation\")\n",
    "desi_stopwords = filterStopWords(\"designation_tkn\", \"designation_tkn\"\n",
    "                                 ,lang=lang,addendum=addendum)\n",
    "\n",
    "\n",
    "#on tokenise et retire les stopwords description\n",
    "\n",
    "desc_tokenize= tokenizeString(\"description_tkn\",\"tr_description\")\n",
    "desc_stopwords = filterStopWords(\"description_tkn\", \"description_tkn\"\n",
    "                                 ,lang=lang,addendum=addendum)\n",
    "\n",
    "\n",
    "\n",
    "merge_desi_desc = mergeFeatures(\"merged\",\"designation_tkn\",\"description_tkn\")\n",
    "desc_mostOccur= mostOccur(\"merged_desi_desc\",\"merged\",700,1000)\n",
    "\n",
    "toString = TokenListToString(\"merged_desi_desc\",\"merged_desi_desc\")\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.initPreprocess()\n",
    "#RktnModel.addPreprocessStep(\"clean designation\",desi_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize designation\",desi_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter designation\",desi_stopwords)  # filtrage des stopwords\n",
    "#RktnModel.addPreprocessStep(\"lemmitize designation\",desi_lemmatize)  # Lemmatisation designation\n",
    "\n",
    "#RktnModel.addPreprocessStep(\"clean description\",desc_filter) # filtre tous les caracteres et mots incorrects\n",
    "RktnModel.addPreprocessStep(\"tokenize description\",desc_tokenize) # transformation en token\n",
    "RktnModel.addPreprocessStep(\"filter description\",desc_stopwords)  # filtrage des stopwords\n",
    "#RktnModel.addPreprocessStep(\"lemmitize description\",desc_lemmatize)  # Lemmatisation\n",
    "RktnModel.addPreprocessStep(\"merge designation and desc\",merge_desi_desc) # merge desi&desc en un seul champ\n",
    "\n",
    "\n",
    "\n",
    "RktnModel.addPreprocessStep(\"desc reduce\",desc_mostOccur)         # reduction taille desc\n",
    "RktnModel.addPreprocessStep(\"tostring\",toString) # convertit en chaine de caracteres\n",
    "\n",
    "RktnModel.preprocess()\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>imgname</th>\n",
       "      <th>desi_langue</th>\n",
       "      <th>tr_designation</th>\n",
       "      <th>desc_langue</th>\n",
       "      <th>tr_description</th>\n",
       "      <th>designation_tkn</th>\n",
       "      <th>description_tkn</th>\n",
       "      <th>merged</th>\n",
       "      <th>merged_desi_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>image_1263597046_product_3804725264.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[olivia, personalisiertes, notizbuch, 150, sei...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[olivia, personalisiertes, notizbuch, 150, sei...</td>\n",
       "      <td>olivia personalisiertes notizbuch 150 seiten p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>image_1008141237_product_436067568.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[journal, arts, 133, 28, 09, 2001, l', art, ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[journal, arts, 133, 28, 09, 2001, l', art, ma...</td>\n",
       "      <td>journal arts 133 28 09 2001 l' art marche salo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>image_938777978_product_201115110.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>fr</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>[pilot, style, touch, pen, marque, speedlink, ...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>image_457047496_product_50418756.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>fr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "      <td>peluche donald europe disneyland 2000 marionne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des idees de grandeur. Il veut organiser...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>image_1077757786_product_278535884.jpg</td>\n",
       "      <td>fr</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>fr</td>\n",
       "      <td>Luc a des idees de grandeur. Il veut organiser...</td>\n",
       "      <td>[guerre, tuques]</td>\n",
       "      <td>[luc, idees, grandeur, veut, organiser, jeu, g...</td>\n",
       "      <td>[guerre, tuques, luc, idees, grandeur, veut, o...</td>\n",
       "      <td>guerre tuques luc idees grandeur veut organise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des idees de grandeur. Il veut organiser...   278535884  1077757786   \n",
       "\n",
       "   prdtypecode                                  imgname desi_langue  \\\n",
       "0           10  image_1263597046_product_3804725264.jpg          fr   \n",
       "1         2280   image_1008141237_product_436067568.jpg          fr   \n",
       "2           50    image_938777978_product_201115110.jpg          fr   \n",
       "3         1280     image_457047496_product_50418756.jpg          fr   \n",
       "4         2705   image_1077757786_product_278535884.jpg          fr   \n",
       "\n",
       "                                      tr_designation desc_langue  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...          fr   \n",
       "1  Journal Des Arts (Le) Ndeg 133 Du 28/09/2001 -...          fr   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...          fr   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...          fr   \n",
       "4                               La Guerre Des Tuques          fr   \n",
       "\n",
       "                                      tr_description  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   \n",
       "3                                                NaN   \n",
       "4  Luc a des idees de grandeur. Il veut organiser...   \n",
       "\n",
       "                                     designation_tkn  \\\n",
       "0  [olivia, personalisiertes, notizbuch, 150, sei...   \n",
       "1  [journal, arts, 133, 28, 09, 2001, l', art, ma...   \n",
       "2  [grand, stylet, ergonomique, bleu, gamepad, ni...   \n",
       "3  [peluche, donald, europe, disneyland, 2000, ma...   \n",
       "4                                   [guerre, tuques]   \n",
       "\n",
       "                                     description_tkn  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [pilot, style, touch, pen, marque, speedlink, ...   \n",
       "3                                                 []   \n",
       "4  [luc, idees, grandeur, veut, organiser, jeu, g...   \n",
       "\n",
       "                                              merged  \\\n",
       "0  [olivia, personalisiertes, notizbuch, 150, sei...   \n",
       "1  [journal, arts, 133, 28, 09, 2001, l', art, ma...   \n",
       "2  [grand, stylet, ergonomique, bleu, gamepad, ni...   \n",
       "3  [peluche, donald, europe, disneyland, 2000, ma...   \n",
       "4  [guerre, tuques, luc, idees, grandeur, veut, o...   \n",
       "\n",
       "                                    merged_desi_desc  \n",
       "0  olivia personalisiertes notizbuch 150 seiten p...  \n",
       "1  journal arts 133 28 09 2001 l' art marche salo...  \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...  \n",
       "3  peluche donald europe disneyland 2000 marionne...  \n",
       "4  guerre tuques luc idees grandeur veut organise...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(RktnModel.data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calcul des tailles des champs texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 11:11:53.943345: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-25 11:11:53.966277: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9511] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-09-25 11:11:53.966299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-09-25 11:11:53.966312: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-25 11:11:53.971373: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-25 11:11:54.495421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import RktnChallenge.RktnModel\n",
    "importlib.reload(RktnChallenge.RktnModel)\n",
    "from RktnChallenge.RktnModel import ModelTrainer\n",
    "\n",
    "import pandas as pd\n",
    "nom_sauvegarde=\"embeddings_final.csv\"\n",
    "RktnModel = ModelTrainer(nom_sauvegarde)\n",
    "RktnModel.data['description_length'] = RktnModel.data['description'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "RktnModel.data['designation_length'] = RktnModel.data['designation'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "\n",
    "min_val = RktnModel.data['description_length'].min()\n",
    "max_val = RktnModel.data['description_length'].max()\n",
    "RktnModel.data['description_length_normalized'] = (RktnModel.data['description_length'] - min_val) / (max_val - min_val)\n",
    "\n",
    "min_val = RktnModel.data['designation_length'].min()\n",
    "max_val = RktnModel.data['designation_length'].max()\n",
    "RktnModel.data['designation_length_normalized'] = (RktnModel.data['designation_length'] - min_val) / (max_val - min_val)\n",
    "\n",
    "RktnModel.saveCSV(nom_sauvegarde)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
